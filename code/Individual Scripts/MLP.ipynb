{"cells":[{"cell_type":"markdown","metadata":{"id":"PjgTjykNKZjk"},"source":["### Multilayer Perceptron (MLP) for multi-class softmax classification (Keras Tensorflow)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xfZL0APWKl5n","executionInfo":{"status":"ok","timestamp":1714919526271,"user_tz":-330,"elapsed":58258,"user":{"displayName":"Abdul D","userId":"13894101110050450996"}},"outputId":"7557466c-46e4-45d6-8b19-7e2b3bd6e838"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ykDuDZknKZjn","executionInfo":{"status":"ok","timestamp":1714919745816,"user_tz":-330,"elapsed":54296,"user":{"displayName":"Abdul D","userId":"13894101110050450996"}},"outputId":"9e46508f-771b-4f73-fa2e-6c6a0c7cdc0a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/300\n","16/16 [==============================] - 1s 16ms/step - loss: 33.7905 - accuracy: 0.3494\n","Epoch 2/300\n","16/16 [==============================] - 0s 15ms/step - loss: 12.0668 - accuracy: 0.4357\n","Epoch 3/300\n","16/16 [==============================] - 0s 16ms/step - loss: 7.0326 - accuracy: 0.4588\n","Epoch 4/300\n","16/16 [==============================] - 0s 13ms/step - loss: 4.4110 - accuracy: 0.5311\n","Epoch 5/300\n","16/16 [==============================] - 0s 13ms/step - loss: 2.5633 - accuracy: 0.5853\n","Epoch 6/300\n","16/16 [==============================] - 0s 13ms/step - loss: 2.0832 - accuracy: 0.5974\n","Epoch 7/300\n","16/16 [==============================] - 0s 14ms/step - loss: 1.5433 - accuracy: 0.6004\n","Epoch 8/300\n","16/16 [==============================] - 0s 12ms/step - loss: 1.2253 - accuracy: 0.6446\n","Epoch 9/300\n","16/16 [==============================] - 0s 13ms/step - loss: 1.0356 - accuracy: 0.6386\n","Epoch 10/300\n","16/16 [==============================] - 0s 14ms/step - loss: 0.8531 - accuracy: 0.6737\n","Epoch 11/300\n","16/16 [==============================] - 0s 14ms/step - loss: 0.7964 - accuracy: 0.6737\n","Epoch 12/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.7718 - accuracy: 0.7139\n","Epoch 13/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.7262 - accuracy: 0.7339\n","Epoch 14/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.6326 - accuracy: 0.7470\n","Epoch 15/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.6288 - accuracy: 0.7470\n","Epoch 16/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.5917 - accuracy: 0.7460\n","Epoch 17/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.5253 - accuracy: 0.7811\n","Epoch 18/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.5159 - accuracy: 0.7982\n","Epoch 19/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.5077 - accuracy: 0.8042\n","Epoch 20/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.4344 - accuracy: 0.8153\n","Epoch 21/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.4471 - accuracy: 0.8193\n","Epoch 22/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.4444 - accuracy: 0.8153\n","Epoch 23/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.4298 - accuracy: 0.8373\n","Epoch 24/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.4136 - accuracy: 0.8173\n","Epoch 25/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.3512 - accuracy: 0.8514\n","Epoch 26/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.3973 - accuracy: 0.8363\n","Epoch 27/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.3126 - accuracy: 0.8705\n","Epoch 28/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.3439 - accuracy: 0.8614\n","Epoch 29/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.3375 - accuracy: 0.8655\n","Epoch 30/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.3083 - accuracy: 0.8695\n","Epoch 31/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.3146 - accuracy: 0.8574\n","Epoch 32/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.3265 - accuracy: 0.8695\n","Epoch 33/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.3318 - accuracy: 0.8614\n","Epoch 34/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.3571 - accuracy: 0.8514\n","Epoch 35/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.2754 - accuracy: 0.8865\n","Epoch 36/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.2863 - accuracy: 0.8906\n","Epoch 37/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.2705 - accuracy: 0.8755\n","Epoch 38/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.2799 - accuracy: 0.8815\n","Epoch 39/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.3408 - accuracy: 0.8805\n","Epoch 40/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.2848 - accuracy: 0.8896\n","Epoch 41/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.2717 - accuracy: 0.8835\n","Epoch 42/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.2721 - accuracy: 0.8775\n","Epoch 43/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.2402 - accuracy: 0.8976\n","Epoch 44/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.2515 - accuracy: 0.9016\n","Epoch 45/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.2231 - accuracy: 0.9096\n","Epoch 46/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.2604 - accuracy: 0.9026\n","Epoch 47/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.2274 - accuracy: 0.8976\n","Epoch 48/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.2524 - accuracy: 0.8956\n","Epoch 49/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.2306 - accuracy: 0.8996\n","Epoch 50/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.2742 - accuracy: 0.8996\n","Epoch 51/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.2190 - accuracy: 0.9157\n","Epoch 52/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.2186 - accuracy: 0.9197\n","Epoch 53/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.2581 - accuracy: 0.8855\n","Epoch 54/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.2023 - accuracy: 0.9026\n","Epoch 55/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.2047 - accuracy: 0.9207\n","Epoch 56/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.2309 - accuracy: 0.9137\n","Epoch 57/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.2075 - accuracy: 0.9167\n","Epoch 58/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1956 - accuracy: 0.9066\n","Epoch 59/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.2080 - accuracy: 0.9076\n","Epoch 60/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.2018 - accuracy: 0.9177\n","Epoch 61/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1954 - accuracy: 0.9237\n","Epoch 62/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.2089 - accuracy: 0.9116\n","Epoch 63/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1777 - accuracy: 0.9277\n","Epoch 64/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1886 - accuracy: 0.9157\n","Epoch 65/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1887 - accuracy: 0.9116\n","Epoch 66/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.2035 - accuracy: 0.9066\n","Epoch 67/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1791 - accuracy: 0.9147\n","Epoch 68/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1973 - accuracy: 0.9207\n","Epoch 69/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.2291 - accuracy: 0.9177\n","Epoch 70/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1719 - accuracy: 0.9247\n","Epoch 71/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1918 - accuracy: 0.9207\n","Epoch 72/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1796 - accuracy: 0.9337\n","Epoch 73/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1898 - accuracy: 0.9207\n","Epoch 74/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1786 - accuracy: 0.9247\n","Epoch 75/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1751 - accuracy: 0.9247\n","Epoch 76/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1947 - accuracy: 0.9167\n","Epoch 77/300\n","16/16 [==============================] - 0s 14ms/step - loss: 0.1773 - accuracy: 0.9197\n","Epoch 78/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1586 - accuracy: 0.9357\n","Epoch 79/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1941 - accuracy: 0.9167\n","Epoch 80/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1799 - accuracy: 0.9187\n","Epoch 81/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1687 - accuracy: 0.9187\n","Epoch 82/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1810 - accuracy: 0.9297\n","Epoch 83/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1662 - accuracy: 0.9277\n","Epoch 84/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1681 - accuracy: 0.9297\n","Epoch 85/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1622 - accuracy: 0.9327\n","Epoch 86/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1719 - accuracy: 0.9247\n","Epoch 87/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1474 - accuracy: 0.9418\n","Epoch 88/300\n","16/16 [==============================] - 0s 15ms/step - loss: 0.2093 - accuracy: 0.9287\n","Epoch 89/300\n","16/16 [==============================] - 0s 14ms/step - loss: 0.1394 - accuracy: 0.9347\n","Epoch 90/300\n","16/16 [==============================] - 0s 14ms/step - loss: 0.1664 - accuracy: 0.9267\n","Epoch 91/300\n","16/16 [==============================] - 0s 16ms/step - loss: 0.1880 - accuracy: 0.9297\n","Epoch 92/300\n","16/16 [==============================] - 0s 16ms/step - loss: 0.1688 - accuracy: 0.9307\n","Epoch 93/300\n","16/16 [==============================] - 0s 17ms/step - loss: 0.1821 - accuracy: 0.9347\n","Epoch 94/300\n","16/16 [==============================] - 0s 15ms/step - loss: 0.1429 - accuracy: 0.9317\n","Epoch 95/300\n","16/16 [==============================] - 0s 16ms/step - loss: 0.1921 - accuracy: 0.9157\n","Epoch 96/300\n","16/16 [==============================] - 0s 15ms/step - loss: 0.1622 - accuracy: 0.9388\n","Epoch 97/300\n","16/16 [==============================] - 0s 15ms/step - loss: 0.1615 - accuracy: 0.9317\n","Epoch 98/300\n","16/16 [==============================] - 0s 14ms/step - loss: 0.1498 - accuracy: 0.9498\n","Epoch 99/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1503 - accuracy: 0.9367\n","Epoch 100/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1441 - accuracy: 0.9398\n","Epoch 101/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.1558 - accuracy: 0.9388\n","Epoch 102/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1918 - accuracy: 0.9317\n","Epoch 103/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1689 - accuracy: 0.9257\n","Epoch 104/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.1642 - accuracy: 0.9378\n","Epoch 105/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1280 - accuracy: 0.9538\n","Epoch 106/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1465 - accuracy: 0.9438\n","Epoch 107/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1594 - accuracy: 0.9388\n","Epoch 108/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1507 - accuracy: 0.9297\n","Epoch 109/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1410 - accuracy: 0.9398\n","Epoch 110/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1394 - accuracy: 0.9418\n","Epoch 111/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1579 - accuracy: 0.9438\n","Epoch 112/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1565 - accuracy: 0.9367\n","Epoch 113/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1412 - accuracy: 0.9367\n","Epoch 114/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1449 - accuracy: 0.9388\n","Epoch 115/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1141 - accuracy: 0.9578\n","Epoch 116/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.1494 - accuracy: 0.9357\n","Epoch 117/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1626 - accuracy: 0.9337\n","Epoch 118/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1579 - accuracy: 0.9388\n","Epoch 119/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1220 - accuracy: 0.9478\n","Epoch 120/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1246 - accuracy: 0.9478\n","Epoch 121/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1397 - accuracy: 0.9438\n","Epoch 122/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1634 - accuracy: 0.9327\n","Epoch 123/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1589 - accuracy: 0.9327\n","Epoch 124/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1575 - accuracy: 0.9418\n","Epoch 125/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1478 - accuracy: 0.9408\n","Epoch 126/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1408 - accuracy: 0.9458\n","Epoch 127/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1635 - accuracy: 0.9347\n","Epoch 128/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1358 - accuracy: 0.9438\n","Epoch 129/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1385 - accuracy: 0.9428\n","Epoch 130/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1377 - accuracy: 0.9428\n","Epoch 131/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1405 - accuracy: 0.9438\n","Epoch 132/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1300 - accuracy: 0.9418\n","Epoch 133/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1179 - accuracy: 0.9518\n","Epoch 134/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1383 - accuracy: 0.9498\n","Epoch 135/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1416 - accuracy: 0.9468\n","Epoch 136/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1252 - accuracy: 0.9478\n","Epoch 137/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1507 - accuracy: 0.9398\n","Epoch 138/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1465 - accuracy: 0.9418\n","Epoch 139/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1325 - accuracy: 0.9378\n","Epoch 140/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1242 - accuracy: 0.9438\n","Epoch 141/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1766 - accuracy: 0.9378\n","Epoch 142/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1302 - accuracy: 0.9528\n","Epoch 143/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1699 - accuracy: 0.9388\n","Epoch 144/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1193 - accuracy: 0.9398\n","Epoch 145/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1416 - accuracy: 0.9408\n","Epoch 146/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1191 - accuracy: 0.9488\n","Epoch 147/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1101 - accuracy: 0.9518\n","Epoch 148/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1366 - accuracy: 0.9468\n","Epoch 149/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1101 - accuracy: 0.9508\n","Epoch 150/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1375 - accuracy: 0.9438\n","Epoch 151/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1292 - accuracy: 0.9538\n","Epoch 152/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1211 - accuracy: 0.9478\n","Epoch 153/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1215 - accuracy: 0.9468\n","Epoch 154/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.1431 - accuracy: 0.9458\n","Epoch 155/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1465 - accuracy: 0.9388\n","Epoch 156/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1196 - accuracy: 0.9508\n","Epoch 157/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1355 - accuracy: 0.9448\n","Epoch 158/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1382 - accuracy: 0.9548\n","Epoch 159/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1302 - accuracy: 0.9468\n","Epoch 160/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1412 - accuracy: 0.9428\n","Epoch 161/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1404 - accuracy: 0.9458\n","Epoch 162/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1305 - accuracy: 0.9498\n","Epoch 163/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1271 - accuracy: 0.9418\n","Epoch 164/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1300 - accuracy: 0.9498\n","Epoch 165/300\n","16/16 [==============================] - 0s 17ms/step - loss: 0.1202 - accuracy: 0.9448\n","Epoch 166/300\n","16/16 [==============================] - 0s 14ms/step - loss: 0.1519 - accuracy: 0.9367\n","Epoch 167/300\n","16/16 [==============================] - 0s 14ms/step - loss: 0.1327 - accuracy: 0.9518\n","Epoch 168/300\n","16/16 [==============================] - 0s 17ms/step - loss: 0.1187 - accuracy: 0.9488\n","Epoch 169/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1105 - accuracy: 0.9528\n","Epoch 170/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1242 - accuracy: 0.9498\n","Epoch 171/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1237 - accuracy: 0.9448\n","Epoch 172/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1513 - accuracy: 0.9418\n","Epoch 173/300\n","16/16 [==============================] - 0s 14ms/step - loss: 0.1034 - accuracy: 0.9538\n","Epoch 174/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1425 - accuracy: 0.9508\n","Epoch 175/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1297 - accuracy: 0.9478\n","Epoch 176/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1097 - accuracy: 0.9578\n","Epoch 177/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1289 - accuracy: 0.9538\n","Epoch 178/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1199 - accuracy: 0.9518\n","Epoch 179/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1290 - accuracy: 0.9408\n","Epoch 180/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1120 - accuracy: 0.9528\n","Epoch 181/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1137 - accuracy: 0.9468\n","Epoch 182/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1216 - accuracy: 0.9548\n","Epoch 183/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1577 - accuracy: 0.9428\n","Epoch 184/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1038 - accuracy: 0.9588\n","Epoch 185/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1549 - accuracy: 0.9458\n","Epoch 186/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1221 - accuracy: 0.9548\n","Epoch 187/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1285 - accuracy: 0.9508\n","Epoch 188/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1302 - accuracy: 0.9468\n","Epoch 189/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1371 - accuracy: 0.9428\n","Epoch 190/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1063 - accuracy: 0.9528\n","Epoch 191/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1361 - accuracy: 0.9498\n","Epoch 192/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1007 - accuracy: 0.9608\n","Epoch 193/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1185 - accuracy: 0.9518\n","Epoch 194/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1113 - accuracy: 0.9488\n","Epoch 195/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1259 - accuracy: 0.9568\n","Epoch 196/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1195 - accuracy: 0.9528\n","Epoch 197/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1106 - accuracy: 0.9508\n","Epoch 198/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1377 - accuracy: 0.9478\n","Epoch 199/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0976 - accuracy: 0.9578\n","Epoch 200/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.1088 - accuracy: 0.9588\n","Epoch 201/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.1164 - accuracy: 0.9488\n","Epoch 202/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.1333 - accuracy: 0.9508\n","Epoch 203/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1345 - accuracy: 0.9528\n","Epoch 204/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1105 - accuracy: 0.9468\n","Epoch 205/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.1228 - accuracy: 0.9528\n","Epoch 206/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.1395 - accuracy: 0.9468\n","Epoch 207/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1247 - accuracy: 0.9518\n","Epoch 208/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1125 - accuracy: 0.9468\n","Epoch 209/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.1016 - accuracy: 0.9578\n","Epoch 210/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.1012 - accuracy: 0.9558\n","Epoch 211/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1297 - accuracy: 0.9568\n","Epoch 212/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1119 - accuracy: 0.9578\n","Epoch 213/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1142 - accuracy: 0.9468\n","Epoch 214/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1362 - accuracy: 0.9438\n","Epoch 215/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.1178 - accuracy: 0.9548\n","Epoch 216/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1257 - accuracy: 0.9518\n","Epoch 217/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.1131 - accuracy: 0.9538\n","Epoch 218/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1093 - accuracy: 0.9588\n","Epoch 219/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.1090 - accuracy: 0.9618\n","Epoch 220/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.1046 - accuracy: 0.9518\n","Epoch 221/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.0955 - accuracy: 0.9538\n","Epoch 222/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.1171 - accuracy: 0.9538\n","Epoch 223/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1318 - accuracy: 0.9518\n","Epoch 224/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1149 - accuracy: 0.9498\n","Epoch 225/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0952 - accuracy: 0.9588\n","Epoch 226/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1109 - accuracy: 0.9558\n","Epoch 227/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1122 - accuracy: 0.9649\n","Epoch 228/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1185 - accuracy: 0.9548\n","Epoch 229/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1249 - accuracy: 0.9488\n","Epoch 230/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1283 - accuracy: 0.9518\n","Epoch 231/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1046 - accuracy: 0.9629\n","Epoch 232/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1143 - accuracy: 0.9568\n","Epoch 233/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.1142 - accuracy: 0.9538\n","Epoch 234/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.1034 - accuracy: 0.9629\n","Epoch 235/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1059 - accuracy: 0.9618\n","Epoch 236/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1343 - accuracy: 0.9558\n","Epoch 237/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1206 - accuracy: 0.9498\n","Epoch 238/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1032 - accuracy: 0.9568\n","Epoch 239/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1210 - accuracy: 0.9528\n","Epoch 240/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1422 - accuracy: 0.9498\n","Epoch 241/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1126 - accuracy: 0.9548\n","Epoch 242/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1055 - accuracy: 0.9548\n","Epoch 243/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1093 - accuracy: 0.9558\n","Epoch 244/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1338 - accuracy: 0.9468\n","Epoch 245/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1222 - accuracy: 0.9598\n","Epoch 246/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1074 - accuracy: 0.9588\n","Epoch 247/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1028 - accuracy: 0.9608\n","Epoch 248/300\n","16/16 [==============================] - 0s 16ms/step - loss: 0.1207 - accuracy: 0.9538\n","Epoch 249/300\n","16/16 [==============================] - 0s 16ms/step - loss: 0.0988 - accuracy: 0.9558\n","Epoch 250/300\n","16/16 [==============================] - 0s 15ms/step - loss: 0.0880 - accuracy: 0.9618\n","Epoch 251/300\n","16/16 [==============================] - 0s 15ms/step - loss: 0.1148 - accuracy: 0.9629\n","Epoch 252/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1005 - accuracy: 0.9498\n","Epoch 253/300\n","16/16 [==============================] - 0s 14ms/step - loss: 0.0939 - accuracy: 0.9629\n","Epoch 254/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1242 - accuracy: 0.9558\n","Epoch 255/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1052 - accuracy: 0.9578\n","Epoch 256/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1107 - accuracy: 0.9488\n","Epoch 257/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1161 - accuracy: 0.9588\n","Epoch 258/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1082 - accuracy: 0.9588\n","Epoch 259/300\n","16/16 [==============================] - 0s 12ms/step - loss: 0.1095 - accuracy: 0.9548\n","Epoch 260/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.0902 - accuracy: 0.9588\n","Epoch 261/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1088 - accuracy: 0.9629\n","Epoch 262/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1337 - accuracy: 0.9488\n","Epoch 263/300\n","16/16 [==============================] - 0s 14ms/step - loss: 0.0953 - accuracy: 0.9548\n","Epoch 264/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.0830 - accuracy: 0.9649\n","Epoch 265/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.0976 - accuracy: 0.9558\n","Epoch 266/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.1696 - accuracy: 0.9458\n","Epoch 267/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.0828 - accuracy: 0.9629\n","Epoch 268/300\n","16/16 [==============================] - 0s 15ms/step - loss: 0.0972 - accuracy: 0.9598\n","Epoch 269/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1157 - accuracy: 0.9578\n","Epoch 270/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1486 - accuracy: 0.9528\n","Epoch 271/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0711 - accuracy: 0.9659\n","Epoch 272/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0970 - accuracy: 0.9578\n","Epoch 273/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1501 - accuracy: 0.9428\n","Epoch 274/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1412 - accuracy: 0.9538\n","Epoch 275/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1103 - accuracy: 0.9598\n","Epoch 276/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1046 - accuracy: 0.9588\n","Epoch 277/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0953 - accuracy: 0.9588\n","Epoch 278/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1234 - accuracy: 0.9558\n","Epoch 279/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0928 - accuracy: 0.9598\n","Epoch 280/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0980 - accuracy: 0.9639\n","Epoch 281/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1130 - accuracy: 0.9558\n","Epoch 282/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1008 - accuracy: 0.9588\n","Epoch 283/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1164 - accuracy: 0.9558\n","Epoch 284/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.0866 - accuracy: 0.9558\n","Epoch 285/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1263 - accuracy: 0.9598\n","Epoch 286/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.0900 - accuracy: 0.9629\n","Epoch 287/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1109 - accuracy: 0.9518\n","Epoch 288/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1094 - accuracy: 0.9598\n","Epoch 289/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.1050 - accuracy: 0.9608\n","Epoch 290/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1780 - accuracy: 0.9568\n","Epoch 291/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.0759 - accuracy: 0.9639\n","Epoch 292/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.0981 - accuracy: 0.9608\n","Epoch 293/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0991 - accuracy: 0.9629\n","Epoch 294/300\n","16/16 [==============================] - 0s 13ms/step - loss: 0.0996 - accuracy: 0.9558\n","Epoch 295/300\n","16/16 [==============================] - 0s 10ms/step - loss: 0.1056 - accuracy: 0.9618\n","Epoch 296/300\n","16/16 [==============================] - 0s 11ms/step - loss: 0.0922 - accuracy: 0.9618\n","Epoch 297/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1200 - accuracy: 0.9588\n","Epoch 298/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.0997 - accuracy: 0.9608\n","Epoch 299/300\n","16/16 [==============================] - 0s 9ms/step - loss: 0.1094 - accuracy: 0.9598\n","Epoch 300/300\n","16/16 [==============================] - 0s 8ms/step - loss: 0.0811 - accuracy: 0.9679\n","7/7 [==============================] - 0s 4ms/step - loss: 0.5733 - accuracy: 0.9206\n"]}],"source":["# coding= UTF-8\n","import numpy as np\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation\n","from keras.optimizers import SGD\n","from sklearn.model_selection import train_test_split\n","\n","# Fix random seed number\n","np.random.seed(7)\n","\n","# Load the data\n","X = np.load('/content/drive/MyDrive/Speaker_Fluency/data/feat.npy')\n","y = np.load('/content/drive/MyDrive/Speaker_Fluency/data/label.npy').ravel() #Return a contiguous flattened array.\n","\n","number_of_features = len(X[1]) #This is variable with each run\n","number_of_classes = 3\n","\n","# Sample data randomly\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) #70% Train, 30% Test\n","\n","# Neural Network Architecture\n","model = Sequential() # Define Sequential model\n","\n","# Using relu on the first two layers and softmax on the output layer\n","\n","# 1st Layer\n","#N neurons, Number_Fatures-dimensional vectors\n","model.add(Dense(512, input_dim=number_of_features, activation='relu')) #32, 64, 128, 256, 512, 1024\n","model.add(Dropout(0.5))\n","\n","# 2nd Layer\n","model.add(Dense(512, activation='relu'))\n","model.add(Dropout(0.5))\n","\n","# 3rd Layer. Output 3 neurons corresponding the number of classes\n","# The sigmoid function is used for the two-class logistic regression,\n","# whereas the softmax function is used for the multiclass logistic regression\n","model.add(Dense(number_of_classes, activation='softmax'))\n","\n","# Model Compilation. Loss for multi-class classification problem\n","sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n","rmsprop = 'rmsprop'\n","adam = 'adam'\n","model.compile(loss='categorical_crossentropy',\n","              optimizer= rmsprop, #rmsprop better than sgd\n","              metrics=['accuracy'])\n","\n","# Convert labels to categorical one-hot encoding\n","y_train = keras.utils.to_categorical(y_train-1, num_classes= number_of_classes) # Convert class vector into binary Matrix\n","y_test = keras.utils.to_categorical(y_test-1, num_classes= number_of_classes)\n","\n","# Train and test\n","model.fit(X_train, y_train, epochs=300, batch_size=64) #batch 32, 64, 128, 256, 512\n","score, acc = model.evaluate(X_test, y_test, batch_size=64)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QsFUOEADKZjp","executionInfo":{"status":"ok","timestamp":1714919773072,"user_tz":-330,"elapsed":7,"user":{"displayName":"Abdul D","userId":"13894101110050450996"}},"outputId":"fce222d8-b79e-4e5c-af99-630c019a0a5f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test score: 0.5732743144035339\n","Test accuracy: 0.9205607771873474\n"]}],"source":["print ('Test score:', score)\n","print ('Test accuracy:', acc)"]}],"metadata":{"kernelspec":{"display_name":"Python2.7 Conda2","language":"python","name":"anaconda2_py27"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.14"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}